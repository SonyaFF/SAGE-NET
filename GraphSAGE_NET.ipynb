{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0\n",
        "!pip install torch_geometric dgl dill"
      ],
      "metadata": {
        "id": "g-ExXNepLkoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit dnc"
      ],
      "metadata": {
        "id": "0ihtWbSPLtEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bzQFki6_iiLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "55a66727-92a5-4796-a902-0e3402705d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.3.0+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 587, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 148, in __call__\n",
            "    content_file.write(chunk)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 970, in handle\n",
            "    self.release()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 924, in release\n",
            "    self.lock.release()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dill'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0274e4f50d2b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch_geometric dgl dill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dill'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import dill\n",
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import math\n",
        "import glob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFC8gREYD7u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################                     Heterogeneous Graph Construction from EHR Data                      ###########################\n",
        "\n",
        "\n",
        "data = pd.read_pickle('/content/drive/MyDrive/Carmen-main/data/data_final.pkl')\n",
        "\n",
        "\n",
        "node_types = ['patient', 'diagnosis', 'drug']\n",
        "\n",
        "edge_types = [\n",
        "    ('patient', 'diagnosis', {'name': 'has'}),\n",
        "    ('diagnosis', 'drug', {'name': 'treated_by'},\n",
        "     'patient','drug' , {'name':'prescriped'})\n",
        "]\n",
        "\n",
        "unique_subject_ids = data['SUBJECT_ID'].unique()\n",
        "unique_icd9_codes = data['ICD9_CODE'].explode().unique()\n",
        "unique_ndc = data['NDC'].explode().unique()\n",
        "\n",
        "#selected_subject_ids = unique_subject_ids[:500]\n",
        "# Filter the data to only include the selected subject IDs\n",
        "#filtered_data = data[data['SUBJECT_ID'].isin(selected_subject_ids)]\n",
        "\n",
        "print(f\"Number of unique SUBJECT_ID: {len(unique_subject_ids)}\")\n",
        "#print(f\"Number of unique SUBJECT_ID: {len(selected_subject_ids)}\")\n",
        "\n",
        "print(f\"Number of unique ICD9_CODE: {len(unique_icd9_codes)}\")\n",
        "print(f\"Number of unique NDC: {len(unique_ndc)}\")\n",
        "\n",
        "# Load the DDI matrix from the file  'ddi_A_final.pkl'\n",
        "ddi_A_final ='/content/drive/MyDrive/Carmen-main/data/ddi_A_final.pkl'\n",
        "ddi_matrix = dill.load(open(ddi_A_final, 'rb'))\n",
        "\n",
        "# Print the DDI matrix\n",
        "print(\"DDI Matrix:\")\n",
        "print(ddi_matrix)\n",
        "\n",
        "print(f\"Number of drugs in the DDI matrix: {len(ddi_matrix)}\")\n",
        "\n",
        "drugs_with_interactions = set()\n",
        "for i in range(len(ddi_matrix)):\n",
        "    for j in range(len(ddi_matrix[i])):\n",
        "        if ddi_matrix[i][j] == 1:\n",
        "            drugs_with_interactions.add(i)\n",
        "            drugs_with_interactions.add(j)\n",
        "\n",
        "# Print the number of drugs with interactions\n",
        "print(f\"Number of drugs with interactions: {len(drugs_with_interactions)}\")\n",
        "\n",
        "def Load_Into_Graph(data):\n",
        "    # Create an empty graph\n",
        "    G = nx.Graph()\n",
        "    patient_diagnosis_edges = 0\n",
        "    diagnosis_drug_edges = 0\n",
        "    drug_drug_edges = 0\n",
        "    patient_drug_edges = 0\n",
        "\n",
        "    # Add nodes for each unique drug NDC code\n",
        "    for ndc_code in unique_ndc:\n",
        "        G.add_node(ndc_code, node_type='drug')\n",
        "\n",
        "    # Add edges for DDIs with negative weight\n",
        "    for i in range(len(ddi_matrix)):\n",
        "        for j in range(i+1, len(ddi_matrix[i])):  # Start from i+1 to avoid double counting\n",
        "            if ddi_matrix[i][j] == 1:\n",
        "                G.add_edge(unique_ndc[i], unique_ndc[j], weight=-1, edge_type='ddi')\n",
        "                drug_drug_edges += 1\n",
        "\n",
        "    # Add patient nodes\n",
        "    for subject_id in unique_subject_ids:\n",
        "        G.add_node(subject_id, node_type='patient')\n",
        "\n",
        "    # Add diagnosis nodes\n",
        "    for icd9_code in unique_icd9_codes:\n",
        "        G.add_node(icd9_code, node_type='diagnosis')\n",
        "\n",
        "    # Add edges from data\n",
        "    for row in data.itertuples(index=False):\n",
        "        patient = row.SUBJECT_ID\n",
        "        diagnosis = row.ICD9_CODE\n",
        "        drug = row.NDC\n",
        "\n",
        "        # Add patient-diagnosis edges\n",
        "        for icd9_code in diagnosis:\n",
        "            if not G.has_edge(patient, icd9_code):\n",
        "                G.add_edge(patient, icd9_code, edge_type='has')\n",
        "                patient_diagnosis_edges += 1\n",
        "\n",
        "        # Add diagnosis-drug edges\n",
        "        for ndc_code in drug:\n",
        "            if not G.has_edge(icd9_code, ndc_code):\n",
        "                G.add_edge(icd9_code, ndc_code, edge_type='treated_by')\n",
        "                diagnosis_drug_edges += 1\n",
        "\n",
        "        # Add patient-drug edges\n",
        "        for ndc_code in drug:\n",
        "            if not G.has_edge(patient, ndc_code):\n",
        "                G.add_edge(patient, ndc_code, edge_type='prescribed')\n",
        "                patient_drug_edges += 1\n",
        "\n",
        "    # Print the number of nodes and edges\n",
        "    print(f\"Number of nodes in the graph: {G.number_of_nodes()}\")\n",
        "    print(f\"Total number of edges in the graph: {G.number_of_edges()}\")\n",
        "    print(f\"Patient-Diagnosis edges: {patient_diagnosis_edges}\")\n",
        "    print(f\"Diagnosis-Drug edges: {diagnosis_drug_edges}\")\n",
        "    print(f\"Drug-Drug edges: {drug_drug_edges}\")\n",
        "    print(f\"Patient-Drug edges: {patient_drug_edges}\")\n",
        "\n",
        "    return G\n",
        "\n",
        "# Load the graph\n",
        "G = Load_Into_Graph(data)\n",
        "\n",
        "######################################                META-PATH  Construction          ###################################\n",
        "\n",
        "def Heterogeneous_Graph(data):\n",
        "    # Populate the heterogeneous graph\n",
        "\n",
        "    G= Load_Into_Graph(data)\n",
        "    # Define the meta-paths\n",
        "    meta_paths = [\n",
        "        ['patient', 'diagnosis', 'drug'],\n",
        "        ['patient', 'diagnosis', 'patient'],\n",
        "        ['diagnosis', 'drug', 'diagnosis'],\n",
        "        ['drug', 'diagnosis', 'patient', 'diagnosis', 'drug'],\n",
        "        ['patient', 'diagnosis', 'drug', 'diagnosis', 'patient']\n",
        "    ]\n",
        "\n",
        "    return G\n",
        "# Meta-path: ['patient', 'diagnosis', 'drug']\n",
        "def patient_diagnosis_drug(G):\n",
        "    count = 0\n",
        "    for patient in G.nodes():\n",
        "        if 'node_type' in G.nodes[patient] and G.nodes[patient]['node_type'] == 'patient':\n",
        "            for diagnosis in G.neighbors(patient):\n",
        "                if 'node_type' in G.nodes[diagnosis] and G.nodes[diagnosis]['node_type'] == 'diagnosis':\n",
        "                    for drug in G.neighbors(diagnosis):\n",
        "                        if G.nodes[drug]['node_type'] == 'drug':\n",
        "                            yield [patient, diagnosis, drug]\n",
        "\n",
        "print(\"**Meta-path: Patient -> Diagnosis -> Drug**\")\n",
        "count = 0\n",
        "max_paths_to_print = 5\n",
        "\n",
        "for path in patient_diagnosis_drug(G):\n",
        "    print(\" -> \".join(str(node) for node in path))\n",
        "    count += 1\n",
        "    if count >= max_paths_to_print:\n",
        "        break\n",
        "\n",
        "\n",
        "# Meta-path: ['patient', 'diagnosis', 'patient']\n",
        "def patient_diagnosis_patient(G):\n",
        "    count = 0\n",
        "    for patient1 in G.nodes():\n",
        "        if G.nodes[patient1].get('node_type') == 'patient':\n",
        "            for diagnosis in G.neighbors(patient1):\n",
        "                if G.nodes[diagnosis].get('node_type') == 'diagnosis':\n",
        "                    for patient2 in G.neighbors(diagnosis):\n",
        "                        if G.nodes[patient2].get('node_type') == 'patient' and patient2 != patient1:\n",
        "                            yield [patient1, diagnosis, patient2]\n",
        "\n",
        "\n",
        "print(\"[**Meta-path: Patient -> Diagnosis -> Patient**]\")\n",
        "count = 0\n",
        "max_paths_to_print = 5\n",
        "\n",
        "for path in patient_diagnosis_patient(G):\n",
        "    print(\" -> \".join(str(node) for node in path))\n",
        "    count += 1\n",
        "    if count >= max_paths_to_print:\n",
        "        break\n",
        "\n",
        "# Meta-path: ['diagnosis', 'drug', 'diagnosis']\n",
        "def diagnosis_drug_diagnosis(G):\n",
        "    count = 0\n",
        "    for diagnosis1 in G.nodes():\n",
        "        if G.nodes[diagnosis1].get('node_type') == 'diagnosis':\n",
        "            for drug in G.neighbors(diagnosis1):\n",
        "                if G.nodes[drug].get('node_type') == 'drug':\n",
        "                    for diagnosis2 in G.neighbors(drug):\n",
        "                        if G.nodes[diagnosis2].get('node_type') == 'diagnosis' and diagnosis2 != diagnosis1:\n",
        "                            yield [diagnosis1, drug, diagnosis2]\n",
        "\n",
        "print(\"[**Meta-path: Diagnosis -> Drug -> Diagnosis**]\")\n",
        "count = 0\n",
        "max_paths_to_print = 5\n",
        "\n",
        "for path in diagnosis_drug_diagnosis(G):\n",
        "    print(\" -> \".join(str(node) for node in path))\n",
        "    count += 1\n",
        "    if count >= max_paths_to_print:\n",
        "        break\n",
        "\n",
        "\n",
        "def find_meta_path_drug_diagnosis_patient_diagnosis_drug(G):\n",
        "    for drug1 in G.nodes():\n",
        "        if G.nodes[drug1].get('node_type') == 'drug':\n",
        "            for diagnosis1 in G.neighbors(drug1):\n",
        "                if G.nodes[diagnosis1].get('node_type') == 'diagnosis':\n",
        "                    for patient in G.neighbors(diagnosis1):\n",
        "                        if G.nodes[patient].get('node_type') == 'patient':\n",
        "                            for diagnosis2 in G.neighbors(patient):\n",
        "                                if G.nodes[diagnosis2].get('node_type') == 'diagnosis' and diagnosis2 != diagnosis1:\n",
        "                                    for drug2 in G.neighbors(diagnosis2):\n",
        "                                        if G.nodes[drug2].get('node_type') == 'drug' and drug2 != drug1:\n",
        "                                            yield [drug1, diagnosis1, patient, diagnosis2, drug2]\n",
        "\n",
        "# print paths\n",
        "print(\"Meta-path: drug_diagnosis_patient_diagnosis_drug\")\n",
        "count = 0\n",
        "max_paths_to_print = 5\n",
        "\n",
        "for path in find_meta_path_drug_diagnosis_patient_diagnosis_drug(G):\n",
        "    print(\" -> \".join(str(node) for node in path))\n",
        "    count += 1\n",
        "    if count >= max_paths_to_print:\n",
        "        break\n",
        "\n",
        "def find_meta_path_patient_diagnosis_drug_diagnosis_patient(G):\n",
        "    for patient1 in G.nodes():\n",
        "        if G.nodes[patient1].get('node_type') == 'patient':\n",
        "            for diagnosis1 in G.neighbors(patient1):\n",
        "                if G.nodes[diagnosis1].get('node_type') == 'diagnosis':\n",
        "                    for drug in G.neighbors(diagnosis1):\n",
        "                        if G.nodes[drug].get('node_type') == 'drug':\n",
        "                            for diagnosis2 in G.neighbors(drug):\n",
        "                                if G.nodes[diagnosis2].get('node_type') == 'diagnosis' and diagnosis2 == diagnosis1:\n",
        "                                    for patient2 in G.neighbors(diagnosis2):\n",
        "                                        if G.nodes[patient2].get('node_type') == 'patient' and patient2 != patient1:\n",
        "                                            yield [patient1, diagnosis1, drug, diagnosis2, patient2]\n",
        "\n",
        "#  print paths\n",
        "print(\"Meta-path: patient_diagnosis_drug_diagnosis_patient\")\n",
        "count = 0\n",
        "max_paths_to_print = 5\n",
        "\n",
        "for path in find_meta_path_patient_diagnosis_drug_diagnosis_patient(G):\n",
        "    print(\" -> \".join(str(node) for node in path))\n",
        "    count += 1\n",
        "    if count >= max_paths_to_print:\n",
        "        break\n",
        "\n",
        "Heterogeneous_Graph(data)\n"
      ],
      "metadata": {
        "id": "DkJu9Pcfioyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############               GraphSAGE MODEL                #####################\n",
        "\n",
        "\n",
        "# Load your data\n",
        "data = data_with_labels = pd.read_pickle('/content/drive/MyDrive/Carmen-main/data/data_with_labels.pkl')\n",
        "print(data.columns)\n",
        "\n",
        "# One-hot encode patient features\n",
        "patient_features = pd.get_dummies(data['SUBJECT_ID'].astype(str))\n",
        "unique_patient_data = data.drop_duplicates(subset='SUBJECT_ID')\n",
        "unique_patient_data.set_index('SUBJECT_ID', inplace=True)\n",
        "patient_features = unique_patient_data.join(patient_features)\n",
        "patient_features = patient_features.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# One-hot encode drug features\n",
        "exploded_ndc = data[['SUBJECT_ID', 'NDC']].explode('NDC')\n",
        "drug_features = pd.get_dummies(exploded_ndc['NDC'].astype(str), prefix='NDC')\n",
        "drug_features = drug_features.groupby(exploded_ndc['SUBJECT_ID']).sum()\n",
        "\n",
        "# One-hot encode diagnosis features\n",
        "exploded_icd9 = data[['SUBJECT_ID', 'ICD9_CODE']].explode('ICD9_CODE')\n",
        "diagnosis_features = pd.get_dummies(exploded_icd9['ICD9_CODE'].astype(str), prefix='ICD9')\n",
        "diagnosis_features = diagnosis_features.groupby(exploded_icd9['SUBJECT_ID']).sum()\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "patient_features = scaler.fit_transform(patient_features.fillna(0))\n",
        "drug_features = scaler.fit_transform(drug_features.fillna(0))\n",
        "diagnosis_features = scaler.fit_transform(diagnosis_features.fillna(0))\n",
        "\n",
        "# Create your HeteroData object\n",
        "hetero_data = HeteroData()\n",
        "hetero_data['patient'].x = torch.tensor(patient_features, dtype=torch.float)\n",
        "hetero_data['diagnosis'].x = torch.tensor(diagnosis_features, dtype=torch.float)\n",
        "hetero_data['drug'].x = torch.tensor(drug_features, dtype=torch.float)\n",
        "\n",
        "# Assuming 'G' is  NetworkX graph\n",
        "# Create mappings from node identifiers to integers\n",
        "node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
        "\n",
        "# Define edge types\n",
        "edge_types = [\n",
        "    ('patient', 'diagnosis', 'has'),\n",
        "    ('diagnosis', 'drug', 'treated_by'),\n",
        "    ('drug', 'drug', 'interaction')\n",
        "]\n",
        "\n",
        "# Assuming 'num_classes' is the number of unique drug labels\n",
        "num_classes = len(set.union(*data['NDC_Labels'].apply(set)))\n",
        "\n",
        "# Convert the lists to tensors and pad them to the same length\n",
        "label_tensors = [torch.tensor(labels, dtype=torch.long) for labels in data['NDC_Labels']]\n",
        "target_embeddings = pad_sequence(label_tensors, batch_first=True, padding_value=-1)\n",
        "\n",
        "# Create a binary matrix for multi-label classification\n",
        "target_embeddings_binary = torch.zeros(target_embeddings.size(0), num_classes)\n",
        "for i, label_tensor in enumerate(label_tensors):\n",
        "\n",
        "    valid_indices = label_tensor[label_tensor != -1]\n",
        "    target_embeddings_binary[i, valid_indices] = 1\n",
        "\n",
        "# Create a dictionary mapping 'node_type' to the corresponding binary tensor\n",
        "target_embeddings_binary_dict = {\n",
        "    'drug': target_embeddings_binary\n",
        "}\n",
        "\n",
        "\n",
        "# Convert edges to integer identifiers and add to HeteroData\n",
        "for edge_type in edge_types:\n",
        "    edges = [(node_mapping[u], node_mapping[v]) for u, v, edge_data in G.edges(data=True)\n",
        "             if edge_data.get('edge_type') == edge_type[2]]\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    hetero_data[edge_type[0], edge_type[1]].edge_index = edge_index\n",
        "\n",
        "# Define the GraphSAGE model\n",
        "class GraphSAGENet(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super(GraphSAGENet, self).__init__()\n",
        "        self.conv1 = SAGEConv(patient_features.shape[1], hidden_channels, normalize=True)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, normalize=True)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # Iterate over all node types in the heterogeneous graph data\n",
        "        for node_type in x_dict.keys():\n",
        "            if (node_type, node_type) in edge_index_dict:\n",
        "                x = x_dict[node_type]\n",
        "                edge_index = edge_index_dict[(node_type, node_type)]\n",
        "\n",
        "                # Apply the first convolution layer\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = torch.relu(x)\n",
        "\n",
        "                # Apply the second convolution layer\n",
        "                x = self.conv2(x, edge_index)\n",
        "\n",
        "                # Store the updated node features in the dictionary\n",
        "                x_dict[node_type] = x\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "\n",
        "model = GraphSAGENet(hidden_channels=64, out_channels=num_classes)\n",
        "# Prepare the data for the model\n",
        "x_dict = {ntype: hetero_data[ntype].x for ntype in hetero_data.node_types}\n",
        "edge_index_dict = {(stype, etype, dtype): hetero_data[stype, etype, dtype].edge_index\n",
        "                   for stype, etype, dtype in hetero_data.edge_types}\n",
        "\n",
        "\n",
        "#  for multi-label classification\n",
        "criterion = torch.nn.BCEWithLogitsLoss()  # Adjusted for binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(x_dict, edge_index_dict)\n",
        "\n",
        "\n",
        "# Calculate total loss for the entire graph\n",
        "total_loss = 0\n",
        "for node_type in hetero_data.node_types:\n",
        "    if node_type in predictions:\n",
        "        if node_type in target_embeddings_binary_dict:\n",
        "            target = target_embeddings_binary_dict[node_type][:predictions[node_type].size(0), :]\n",
        "            target.requires_grad = True  # Set requires_grad to True\n",
        "            loss = criterion(predictions[node_type], target)\n",
        "            total_loss += loss\n",
        "\n",
        "# Backpropagate and optimize based on the total loss\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# After training loop\n",
        "embeddings = {node_type: model(x_dict, edge_index_dict)[node_type].detach().numpy() for node_type in hetero_data.node_types}\n",
        "\n",
        "# Print the embeddings for each node type\n",
        "for node_type, emb in embeddings.items():\n",
        "    print(f\"Embeddings for {node_type} nodes:\")\n",
        "    print(emb)\n",
        "\n",
        "\n",
        "\n",
        "# Assuming 'embeddings' is a dictionary containing  node embeddings\n",
        "for node_type, emb in embeddings.items():\n",
        "    # Save the embeddings to a file\n",
        "    np.save(f'{node_type}_embeddings.npy', emb)\n",
        "\n",
        "\n",
        "# To load the embeddings for a specific node type\n",
        "patient_embeddings = np.load('patient_embeddings.npy')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qQLLG2a_Dc5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################  reshape embeddings and save\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Load your embeddings\n",
        "patient_embeddings = np.load('patient_embeddings.npy')\n",
        "diagnosis_embeddings = np.load('diagnosis_embeddings.npy')\n",
        "drug_embeddings = np.load('drug_embeddings.npy')\n",
        "\n",
        " # Apply PCA to reduce feature dimension to 64\n",
        "pca_drug = PCA(n_components=64)\n",
        "drug_embeddings_reduced = pca_drug.fit_transform(drug_embeddings)\n",
        "\n",
        "# Apply PCA to reduce feature dimension to 64\n",
        "pca_patient = PCA(n_components=64)\n",
        "patient_embeddings_reduced = pca_patient.fit_transform(patient_embeddings)\n",
        "\n",
        "drug_embeddings_final = drug_embeddings_reduced[:32, :]\n",
        "patient_embeddings_final = patient_embeddings_reduced[:32, :]\n",
        "\n",
        "# Print the shapes of the embeddings\n",
        "print(\"Shape of patient_embeddings_final:\", patient_embeddings_final.shape)\n",
        "print(\"Shape of reshaped drug_embeddings_final:\", drug_embeddings_final.shape)\n",
        "\n",
        "\n",
        "# Save the reshaped embeddings\n",
        "np.save('patient_embeddings_final.npy', patient_embeddings_final)\n",
        "np.save('diagnosis_embeddings.npy', diagnosis_embeddings)\n",
        "np.save('drug_embeddings_final.npy', drug_embeddings_final)\n",
        "\n",
        "patient_embeddings = np.load('patient_embeddings_final.npy')\n",
        "drug_embeddings_final = np.load('drug_embeddings_final.npy')\n",
        "diagnosis_embeddings = np.load('diagnosis_embeddings.npy')"
      ],
      "metadata": {
        "id": "92Zf5NjAio2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## (preprocess_meta_paths ######################################\n",
        "\n",
        "\n",
        "# Load the embeddings\n",
        "diagnosis_embeddings = np.load('diagnosis_embeddings.npy')\n",
        "drug_embeddings = np.load('drug_embeddings.npy')\n",
        "\n",
        "# Create mappings from diagnosis and drug identifiers to indices\n",
        "diagnosis_to_idx = {diagnosis_id: idx for idx, diagnosis_id in enumerate(unique_icd9_codes)}\n",
        "drug_to_idx = {drug_id: idx for idx, drug_id in enumerate(unique_ndc)}\n",
        "\n",
        "def get_meta_path_embedding(diagnosis_embeddings, drug_embeddings, graph, diagnosis_to_idx, drug_to_idx):\n",
        "    meta_path_embeddings = []\n",
        "\n",
        "    # Iterate over all possible meta-path instances\n",
        "    for diagnosis1 in G.nodes(data='node_type'):\n",
        "        if diagnosis1[1] == 'diagnosis':\n",
        "            for drug in G.neighbors(diagnosis1[0]):\n",
        "                for diagnosis2 in G.neighbors(drug):\n",
        "                    if diagnosis2 != diagnosis1[0]:\n",
        "                        # Get the integer indices for the embeddings\n",
        "                        diagnosis1_idx = diagnosis_to_idx[diagnosis1[0]]\n",
        "                        drug_idx = drug_to_idx[drug]\n",
        "                        diagnosis2_idx = diagnosis_to_idx[diagnosis2]\n",
        "\n",
        "                        # Concatenate embeddings using the indices\n",
        "                        path_embedding = np.concatenate(\n",
        "                            (diagnosis_embeddings[diagnosis1_idx],\n",
        "                             drug_embeddings[drug_idx],\n",
        "                             diagnosis_embeddings[diagnosis2_idx])\n",
        "                        )\n",
        "                        meta_path_embeddings.append(path_embedding)\n",
        "\n",
        "    # Aggregate embeddings by averaging\n",
        "    aggregated_embedding = np.mean(meta_path_embeddings, axis=0)\n",
        "    return aggregated_embedding\n",
        "\n",
        "# Example usage:\n",
        "meta_path_embedding = get_meta_path_embedding(diagnosis_embeddings, drug_embeddings, G, diagnosis_to_idx, drug_to_idx)\n"
      ],
      "metadata": {
        "id": "V8rL0VL4fBdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHUEsrCdfBUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####   Our model with cross attention       ################################\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import MultiheadAttention\n",
        "\n",
        "class SAGENet(nn.Module):\n",
        "    def __init__(self, vocab_size, ehr_adj, ddi_adj, emb_dim=64, device=torch.device('cpu:0'), ddi_in_memory=True):\n",
        "        super(SAGENet, self).__init__()\n",
        "        K = len(vocab_size)\n",
        "        self.K = K\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = device\n",
        "        self.tensor_ddi_adj = torch.FloatTensor(ddi_adj).to(device)\n",
        "        self.ddi_in_memory = ddi_in_memory\n",
        "        self.embeddings = nn.ModuleList(\n",
        "            [nn.Embedding(vocab_size[i], emb_dim) for i in range(K-1)])\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "\n",
        "\n",
        "        self.query = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim * 4, emb_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "        source_nodes, target_nodes = np.where(ehr_adj == 1)\n",
        "        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
        "\n",
        "        # Load precomputed embeddings from GraphSAGE MODEL\n",
        "        self.patient_embeddings = torch.tensor(np.load('/content/drive/MyDrive/Carmen-main/data/patient_embeddings_final.npy'), dtype=torch.float).to(device)\n",
        "        self.drug_embeddings = torch.tensor(np.load('/content/drive/MyDrive/Carmen-main/data/drug_embeddings_final.npy'), dtype=torch.float).to(device)\n",
        "        self.diagnosis_embeddings = torch.tensor(np.load('/content/drive/MyDrive/Carmen-main/data/diagnosis_embeddings.npy'), dtype=torch.float).to(device)\n",
        "        self.ehr_gcn = GCN(voc_size=vocab_size[2], emb_dim=emb_dim, adj=ehr_adj, device=device)\n",
        "\n",
        "        self.ddi_gcn = GCN(voc_size=vocab_size[2], emb_dim=emb_dim, adj=ddi_adj, device=device)\n",
        "        self.inter = nn.Parameter(torch.FloatTensor(1))\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(emb_dim  4 , emb_dim  2),  #############3\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(emb_dim * 2, vocab_size[2])\n",
        "        )\n",
        "\n",
        "        #####Add a cross attention layer\n",
        "        self.num_heads = 4\n",
        "        self.cross_attn = MultiheadAttention(emb_dim, self.num_heads)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input (adm, 3, codes)\n",
        "\n",
        "\n",
        "        patient_embeddings = self.patient_embeddings\n",
        "        drug_embeddings = self.drug_embeddings\n",
        "\n",
        "        #print(\"drug_embeddings shape:\", drug_embeddings.shape)\n",
        "        #print(\"patient_embeddings shape:\", patient_embeddings.shape)\n",
        "        #print(\"ddi_embeddings shape:\", ddi_embeddings.shape)\n",
        "\n",
        "        patient_embeddings_avg = patient_embeddings.mean(dim=0, keepdim=True)\n",
        "        drug_embeddings_avg = drug_embeddings.mean(dim=0, keepdim=True)\n",
        "\n",
        "        #print(\"patient_embeddings-avg shape:\", patient_embeddings_avg.shape)\n",
        "\n",
        "        ddi_embedding =  self.ehr_gcn() - self.ddi_gcn() * self.inter  # (size, dim)\n",
        "        ddi_embedding_avg = ddi_embedding.mean(dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "        patient_embeddings_avg = patient_embeddings_avg.unsqueeze(1)\n",
        "        drug_embeddings_avg  = drug_embeddings_avg .unsqueeze(1)\n",
        "        ddi_embedding_avg = ddi_embedding_avg.unsqueeze(1)\n",
        "\n",
        "        #print(\"Shape of patient_embeddings_avg:\", patient_embeddings_avg.shape)\n",
        "        #print(\"Shape of drug_embeddings_avg:\", drug_embeddings_avg.shape)\n",
        "        #print(\"Shape of ddi_embedding:\", ddi_embedding_avg .shape)\n",
        "\n",
        "        #'''Cross Attention'''#\n",
        "        cross_attn_output, cross_attn_weights = self.cross_attn(patient_embeddings_avg, drug_embeddings_avg, ddi_embedding_avg) # (1, 1, dim), (1, 1, 1)\n",
        "        cross_attn_output = cross_attn_output.transpose(0, 1) # (1, dim)\n",
        "        #print('cross_attn_output=' , cross_attn_output.shape)\n",
        "\n",
        "\n",
        "        output = self.output(torch.cat([ patient_embeddings_avg, drug_embeddings_avg, ddi_embedding_avg, cross_attn_output], dim=-1)) # (1, dim)###############333\n",
        "        #print(\"output shape:\", output.shape)\n",
        "\n",
        "        output = output.squeeze(0)\n",
        "        #print('new_output=', output.shape)\n",
        "\n",
        "        if self.training:\n",
        "            neg_pred_prob = F.sigmoid(output)\n",
        "            neg_pred_prob = neg_pred_prob.t() * neg_pred_prob  # (voc_size, voc_size)\n",
        "            batch_neg = neg_pred_prob.mul(self.tensor_ddi_adj).mean()\n",
        "\n",
        "            return output, batch_neg\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange = 0.1\n",
        "        for item in self.embeddings:\n",
        "            item.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        self.inter.data.uniform_(-initrange, initrange)\n"
      ],
      "metadata": {
        "id": "s1qr3wjSio4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FxUL2Icio7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}